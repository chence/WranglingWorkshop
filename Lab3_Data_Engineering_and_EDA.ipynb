{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d2063a",
   "metadata": {},
   "source": [
    "# Lab3 – Data Engineering & EDA with Python, SQL, and Pandas\n",
    "\n",
    "**Coursework goal:** practice hands-on data engineering by generating synthetic employee data, storing it in a free cloud Postgres database (Neon), loading it with Python, and performing EDA + visualizations.\n",
    "\n",
    "> ✅ **Important:** You must create a free Neon Postgres database and paste your connection details in the **Environment Variables** cell below.  \n",
    "> This notebook is written so you can run it end-to-end after you set those variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ca8f8",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "This notebook uses:\n",
    "- **Neon Postgres** (cloud database)\n",
    "- **psycopg2** for connecting + inserting\n",
    "- **Pandas** for data wrangling\n",
    "- **Faker** for synthetic records\n",
    "- **Matplotlib** for visualizations\n",
    "- **scikit-learn** for scaling\n",
    "\n",
    "Run the install cell once (or ensure these are already installed in your environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d32747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies (uncomment if running in a fresh environment)\n",
    "# !pip install pandas psycopg2-binary faker matplotlib scikit-learn python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7a024",
   "metadata": {},
   "source": [
    "## 1. Data Collection & Database Connection\n",
    "\n",
    "### 1.1 Create a free Neon database\n",
    "1. Go to Neon.tech → create a free project\n",
    "2. Copy your connection info (host, database, user, password, port)\n",
    "3. Set environment variables below (recommended), or paste them directly.\n",
    "\n",
    "**Why environment variables?**  \n",
    "It prevents accidentally committing credentials into GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f48ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option A (recommended): set these in your shell / .env file\n",
    "# export NEON_HOST=\"...\"\n",
    "# export NEON_DB=\"...\"\n",
    "# export NEON_USER=\"...\"\n",
    "# export NEON_PASSWORD=\"...\"\n",
    "# export NEON_PORT=\"5432\"\n",
    "\n",
    "NEON_HOST = os.getenv(\"NEON_HOST\", \"YOUR_NEON_HOST\")\n",
    "NEON_DB = os.getenv(\"NEON_DB\", \"YOUR_NEON_DBNAME\")\n",
    "NEON_USER = os.getenv(\"NEON_USER\", \"YOUR_NEON_USERNAME\")\n",
    "NEON_PASSWORD = os.getenv(\"NEON_PASSWORD\", \"YOUR_NEON_PASSWORD\")\n",
    "NEON_PORT = int(os.getenv(\"NEON_PORT\", \"5432\"))\n",
    "\n",
    "assert \"YOUR_NEON_\" not in (NEON_HOST + NEON_DB + NEON_USER + NEON_PASSWORD), (\n",
    "    \"Please set your Neon credentials (env vars) before running.\"\n",
    ")\n",
    "\n",
    "CONN_INFO = dict(\n",
    "    host=NEON_HOST,\n",
    "    dbname=NEON_DB,\n",
    "    user=NEON_USER,\n",
    "    password=NEON_PASSWORD,\n",
    "    port=NEON_PORT,\n",
    "    sslmode=\"require\",  # Neon typically requires SSL\n",
    ")\n",
    "\n",
    "CONN_INFO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf65df",
   "metadata": {},
   "source": [
    "### 1.2 Connect to Postgres (Neon)\n",
    "\n",
    "We connect using `psycopg2`, then use SQL to create the required table(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def get_conn():\n",
    "    return psycopg2.connect(**CONN_INFO)\n",
    "\n",
    "# quick connection test\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT NOW();\")\n",
    "        print(\"Connected. Server time:\", cur.fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0093a79",
   "metadata": {},
   "source": [
    "### 1.3 Create tables\n",
    "\n",
    "Required table: **employees**\n",
    "\n",
    "We also create a **departments** table to support the *advanced visualization challenge* (join + more complex dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPLOYEES_DDL = '''\n",
    "CREATE TABLE IF NOT EXISTS employees (\n",
    "    employee_id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    position TEXT NOT NULL,\n",
    "    start_date DATE NOT NULL,\n",
    "    salary INTEGER NOT NULL CHECK (salary BETWEEN 60000 AND 200000)\n",
    ");\n",
    "'''\n",
    "\n",
    "DEPARTMENTS_DDL = '''\n",
    "CREATE TABLE IF NOT EXISTS departments (\n",
    "    department_id SERIAL PRIMARY KEY,\n",
    "    department_name TEXT NOT NULL,\n",
    "    location TEXT NOT NULL,\n",
    "    annual_budget INTEGER NOT NULL CHECK (annual_budget BETWEEN 200000 AND 5000000)\n",
    ");\n",
    "'''\n",
    "\n",
    "# Bridge table (employee belongs to a department)\n",
    "EMP_DEPT_DDL = '''\n",
    "CREATE TABLE IF NOT EXISTS employee_departments (\n",
    "    employee_id INTEGER PRIMARY KEY REFERENCES employees(employee_id) ON DELETE CASCADE,\n",
    "    department_id INTEGER NOT NULL REFERENCES departments(department_id) ON DELETE CASCADE\n",
    ");\n",
    "'''\n",
    "\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(EMPLOYEES_DDL)\n",
    "        cur.execute(DEPARTMENTS_DDL)\n",
    "        cur.execute(EMP_DEPT_DDL)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Tables ensured: employees, departments, employee_departments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3b169",
   "metadata": {},
   "source": [
    "### 1.4 Generate & populate synthetic records (≥ 50)\n",
    "\n",
    "We generate **at least 50 employees** using Faker:\n",
    "- IT-related positions\n",
    "- start_date between 2015 and 2024\n",
    "- salary between 60k and 200k\n",
    "\n",
    "Then we insert into Neon Postgres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc93e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "POSITIONS = [\n",
    "    \"Data Engineer\",\n",
    "    \"Data Analyst\",\n",
    "    \"ML Engineer\",\n",
    "    \"Cloud Engineer\",\n",
    "    \"DevOps Engineer\",\n",
    "    \"Backend Developer\",\n",
    "    \"Full Stack Developer\",\n",
    "    \"Database Administrator\",\n",
    "    \"Cybersecurity Analyst\",\n",
    "    \"QA Automation Engineer\",\n",
    "]\n",
    "\n",
    "DEPT_SEED = [\n",
    "    (\"Data Platform\", \"Toronto, ON\"),\n",
    "    (\"Cloud Infrastructure\", \"Waterloo, ON\"),\n",
    "    (\"Security\", \"Ottawa, ON\"),\n",
    "    (\"Product Engineering\", \"Mississauga, ON\"),\n",
    "    (\"Analytics\", \"Montreal, QC\"),\n",
    "]\n",
    "\n",
    "def random_date_2015_2024():\n",
    "    # Faker date_between with explicit bounds\n",
    "    return fake.date_between(start_date=\"2015-01-01\", end_date=\"2024-12-31\")\n",
    "\n",
    "def gen_employees(n=80, start_id=1001):\n",
    "    records = []\n",
    "    for i in range(n):\n",
    "        emp_id = start_id + i\n",
    "        name = fake.name()\n",
    "        position = random.choice(POSITIONS)\n",
    "        start_date = random_date_2015_2024()\n",
    "        salary = random.randint(60000, 200000)\n",
    "        records.append((emp_id, name, position, start_date, salary))\n",
    "    return records\n",
    "\n",
    "def gen_departments():\n",
    "    rows = []\n",
    "    for dept_name, loc in DEPT_SEED:\n",
    "        budget = random.randint(300000, 4000000)\n",
    "        rows.append((dept_name, loc, budget))\n",
    "    return rows\n",
    "\n",
    "employees_rows = gen_employees(n=80, start_id=1001)  # >= 50\n",
    "departments_rows = gen_departments()\n",
    "\n",
    "len(employees_rows), len(departments_rows), employees_rows[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3fbaa",
   "metadata": {},
   "source": [
    "#### Insert into database\n",
    "\n",
    "To make re-runs easy, we:\n",
    "- **TRUNCATE** (clear) tables first\n",
    "- Insert fresh synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Clear dependent tables first\n",
    "        cur.execute(\"TRUNCATE TABLE employee_departments;\")\n",
    "        cur.execute(\"TRUNCATE TABLE employees;\")\n",
    "        cur.execute(\"TRUNCATE TABLE departments RESTART IDENTITY;\")\n",
    "\n",
    "        # Insert departments\n",
    "        execute_values(\n",
    "            cur,\n",
    "            \"INSERT INTO departments (department_name, location, annual_budget) VALUES %s\",\n",
    "            departments_rows,\n",
    "        )\n",
    "\n",
    "        # Insert employees\n",
    "        execute_values(\n",
    "            cur,\n",
    "            \"INSERT INTO employees (employee_id, name, position, start_date, salary) VALUES %s\",\n",
    "            employees_rows,\n",
    "        )\n",
    "\n",
    "        # Assign each employee to a department (random mapping)\n",
    "        cur.execute(\"SELECT department_id FROM departments;\")\n",
    "        dept_ids = [r[0] for r in cur.fetchall()]\n",
    "        emp_dept_rows = [(emp_id, random.choice(dept_ids)) for (emp_id, *_rest) in employees_rows]\n",
    "\n",
    "        execute_values(\n",
    "            cur,\n",
    "            \"INSERT INTO employee_departments (employee_id, department_id) VALUES %s\",\n",
    "            emp_dept_rows,\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Inserted:\", len(employees_rows), \"employees,\", len(departments_rows), \"departments, and employee_department mappings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1001f",
   "metadata": {},
   "source": [
    "### 1.5 Query and load data into a Pandas DataFrame\n",
    "\n",
    "We query the entire employee table and display `df.head()` as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87628b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with get_conn() as conn:\n",
    "    df_emp = pd.read_sql(\"SELECT * FROM employees ORDER BY employee_id;\", conn)\n",
    "\n",
    "df_emp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ec1ee",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section we demonstrate:\n",
    "- **Descriptive statistics**: `.info()`, `.describe()`\n",
    "- **Missing values**: `.isnull().sum()`\n",
    "- Basic sanity checks (salary range, date range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b71893",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "Because the data is synthetic, we expect no missing values, but we still **check and enforce**:\n",
    "- salary within **[60k, 200k]**\n",
    "- start_date between **2015-01-01** and **2024-12-31**\n",
    "- normalize position names (trim spaces, consistent casing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e874c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct dtypes\n",
    "df_emp[\"start_date\"] = pd.to_datetime(df_emp[\"start_date\"])\n",
    "\n",
    "# Normalize positions\n",
    "df_emp[\"position\"] = df_emp[\"position\"].astype(str).str.strip()\n",
    "\n",
    "# Sanity checks\n",
    "salary_outside = df_emp[(df_emp[\"salary\"] < 60000) | (df_emp[\"salary\"] > 200000)]\n",
    "date_outside = df_emp[(df_emp[\"start_date\"] < \"2015-01-01\") | (df_emp[\"start_date\"] > \"2024-12-31\")]\n",
    "\n",
    "salary_outside.shape, date_outside.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3fa8b",
   "metadata": {},
   "source": [
    "## 4. Data Transformation & Feature Engineering\n",
    "\n",
    "We add:\n",
    "- `start_year`: extracted from start_date\n",
    "- `years_of_service`: years from start_date to a reference date (today)\n",
    "\n",
    "These are useful for grouping, trends, and tenure-based analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ef491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = pd.Timestamp(date.today())\n",
    "\n",
    "df_emp[\"start_year\"] = df_emp[\"start_date\"].dt.year\n",
    "df_emp[\"years_of_service\"] = ((today - df_emp[\"start_date\"]).dt.days / 365.25).round(2)\n",
    "\n",
    "df_emp[[\"employee_id\", \"position\", \"start_date\", \"start_year\", \"years_of_service\", \"salary\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fc501",
   "metadata": {},
   "source": [
    "## 5. Scaling\n",
    "\n",
    "We apply scaling to numeric columns to prepare data for modeling and to compare features on a similar scale.\n",
    "\n",
    "Here we scale:\n",
    "- `salary`\n",
    "- `years_of_service`\n",
    "\n",
    "We demonstrate **MinMaxScaler** (0 to 1). (StandardScaler is also valid.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df_emp.copy()\n",
    "\n",
    "df_scaled[[\"salary_scaled\", \"years_scaled\"]] = scaler.fit_transform(df_emp[[\"salary\", \"years_of_service\"]])\n",
    "\n",
    "df_scaled[[\"salary\", \"salary_scaled\", \"years_of_service\", \"years_scaled\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46893ce0",
   "metadata": {},
   "source": [
    "## 6. Visualization 1 (Standard): Average Salary by Position and Start Year\n",
    "\n",
    "**Goal:** grouped bar chart showing the **average salary** by:\n",
    "- `position`\n",
    "- `start_year`\n",
    "\n",
    "This matches the class-style visualization requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pivot for grouped bar chart\n",
    "pivot = (\n",
    "    df_emp\n",
    "    .groupby([\"position\", \"start_year\"])[\"salary\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"position\", columns=\"start_year\", values=\"salary\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "ax = pivot.plot(kind=\"bar\", figsize=(14, 6))\n",
    "ax.set_title(\"Average Salary by Position and Start Year\")\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Average Salary (USD)\")\n",
    "ax.legend(title=\"Start Year\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d13c3",
   "metadata": {},
   "source": [
    "### Findings (Visualization 1)\n",
    "\n",
    "Write 3–5 bullets **in your own words** after you run the plot. Example structure:\n",
    "- Which positions tend to have higher averages?\n",
    "- Are there any years with unusually high/low averages (could be random synthetic variation)?\n",
    "- Any noticeable changes over time?\n",
    "\n",
    "*(Replace the placeholders below with your observations.)*\n",
    "\n",
    "- Observation 1: …\n",
    "- Observation 2: …\n",
    "- Observation 3: …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc4b48",
   "metadata": {},
   "source": [
    "## 7. Visualization 2 (Advanced): Join with Departments + Heatmap\n",
    "\n",
    "### 7.1 Build a more complex dataset (SQL join)\n",
    "We join:\n",
    "- employees\n",
    "- employee_departments\n",
    "- departments\n",
    "\n",
    "Then visualize **average salary** by:\n",
    "- department_name\n",
    "- position\n",
    "\n",
    "We use a **heatmap** (matplotlib `imshow`) to satisfy the advanced visualization requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_conn() as conn:\n",
    "    df_joined = pd.read_sql(\n",
    "        '''\n",
    "        SELECT \n",
    "            e.employee_id,\n",
    "            e.name,\n",
    "            e.position,\n",
    "            e.start_date,\n",
    "            e.salary,\n",
    "            d.department_name,\n",
    "            d.location,\n",
    "            d.annual_budget\n",
    "        FROM employees e\n",
    "        JOIN employee_departments ed ON e.employee_id = ed.employee_id\n",
    "        JOIN departments d ON ed.department_id = d.department_id\n",
    "        ORDER BY e.employee_id;\n",
    "        ''',\n",
    "        conn\n",
    "    )\n",
    "\n",
    "df_joined[\"start_date\"] = pd.to_datetime(df_joined[\"start_date\"])\n",
    "df_joined[\"start_year\"] = df_joined[\"start_date\"].dt.year\n",
    "df_joined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a table of avg salary by department and position\n",
    "heat = (\n",
    "    df_joined\n",
    "    .groupby([\"department_name\", \"position\"])[\"salary\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"department_name\", columns=\"position\", values=\"salary\")\n",
    ")\n",
    "\n",
    "# Heatmap with imshow (no seaborn)\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "im = ax.imshow(heat.values, aspect=\"auto\")\n",
    "\n",
    "ax.set_title(\"Heatmap: Average Salary by Department and Position\")\n",
    "ax.set_xlabel(\"Position\")\n",
    "ax.set_ylabel(\"Department\")\n",
    "\n",
    "ax.set_xticks(np.arange(heat.shape[1]))\n",
    "ax.set_xticklabels(heat.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(heat.shape[0]))\n",
    "ax.set_yticklabels(heat.index)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Average Salary (USD)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8493be8",
   "metadata": {},
   "source": [
    "### Findings (Visualization 2)\n",
    "\n",
    "Write 3–5 bullets **in your own words** after you run the heatmap. Example structure:\n",
    "- Which department + position combinations have the highest averages?\n",
    "- Is one department consistently higher across roles?\n",
    "- Any outliers that might come from random assignment?\n",
    "\n",
    "*(Replace the placeholders below with your observations.)*\n",
    "\n",
    "- Observation 1: …\n",
    "- Observation 2: …\n",
    "- Observation 3: …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87967b37",
   "metadata": {},
   "source": [
    "## 8. Insights & Conclusions\n",
    "\n",
    "Summarize what you learned:\n",
    "- connecting Python ↔ cloud Postgres\n",
    "- generating + inserting data\n",
    "- cleaning / transformation / feature engineering\n",
    "- scaling\n",
    "- building visualizations and extracting insights\n",
    "\n",
    "**Example outline:**\n",
    "- What worked smoothly?\n",
    "- What was tricky?\n",
    "- What you would improve if this were real HR data?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
